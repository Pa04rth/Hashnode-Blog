---
title: "OWASP AIVSS"
slug: owasp-aivss

---

Recently I go through a paper named “AIVSS Scoring System For OWASP Agentic AI Core Security Risks v0.5“ published by OWASP AIVSS , OWASP AI EXCHANGE and OWASP LCNC Top 10 Projects and reading it was definitely worth it .

So I thought to deliver all of my learnings in a form of this blog in crisp and digestible form. Here are my notes :

### EXECUTIVE SUMMARY →

1- Even though Agentic AI is a top strategic technology trend for 2025 , still there is currently no standard definition of the term **Agentic AI.**

2- This document established a working definition of Agentic AI → “*artificial intelligence systems that demonstrate the capacity to pursue goals autonomously, make independent decisions based on environmental reasoning and planning, and interact with external tools, systems, or other agents to effect change within their operational domain*“

3-Agentic AI have a high degree of autonomy and requires very minimal human oversight. Their behaviors are built through dynamic reasoning , planning , memory retrieval and continuous adaptation to changing contexts.

4- Normal AI agents (Chatbots & Learning Models) are only capable of planning and executing task based on abstract prompts or high-level objectives and interacting with external environments such as APIs, command-line interfaces, databases, cloud platforms, and human interfaces.

5-Whereas AI agents can adapt their behavior in response to outputs received or evolving environmental signals . Thus, challenging our conventional security practices.

6-Their unique ability to make independent decisions , manage dynamic identities, delegate tasks, and utilize memory creates unique attack surfaces—especially due to the persistent use of natural language as both an instruction set between humans and agents, and as an inter-agent control layer—that require capabilities beyond traditional security controls.

7-This document focuses on leveraging the **NIST AI Risk Management Framework (AI RMF)**, particularly its Map and Measure functions, to establish a structured approach to understanding and managing risks in agentic AI.

### **PART 1: OWASP AGENTIC AI CORE SECURITY RISKS →**

1- The OWASP Agentic AI Core Security Risks serves as a foundational reference, presenting each of the ten critical vulnerability categories that uniquely affect Agentic AI systems.

2- The risks are listed in decreasing order of severity .For each distinct risk, this section provides a comprehensive description, outlines its associated key dangers and common manifestations, details established prevention and mitigation strategies, and offers example attack scenarios.

Here is the list -

**→ Agentic AI Tool Misuse**

**→ Agent Access Control Violation**

**→ Agent Cascading Failures**

**→ Agent Orchestration and Multi-Agent Exploitation**

**→ Agent Identity Impersonation**

**→ Agent Memory and Context Manipulation**

**→ Insecure Agent Critical Systems Interaction**

**→ Agent Supply Chain and Dependency Attacks**

**→ Agent Untraceability**

**→Agent Goal and Instruction Manipulation**

3- Because Agentic Systems are interconnected by design , it is possible that we find a repetition in entries.

### **AGENTIC AI TOOL MISUSE →**

1. **Agentic AI Tool Misuse** happens when an AI system (called an "agent") uses tools or resources in the real world in a way that causes **bad or harmful results.** Like for example :- if an AI assistant is allowed to send emails and it mistakenly sends sensitive info to the wrong person that would be misuse of its tools. Or even if the tool is used incorrectly (in ways that weren’t intended) , it can cause problems .
    
2. The potential causes are as follows →
    
    → **Compromised integrity of toolchain integrations :** The connection between the AI and the tools it uses is broken, unsafe, or tampered with.
    
    → **Deficiencies in the agent's inferential or logical capabilities or alignment :** The AI doesn’t think things through properly or doesn't follow the right goals
    
    → **Malicious injection or manipulation of tool specifications or schemata :** Someone tricks the AI by secretly changing how the tool works or what it expects.
    
    → **Erroneous parsing or semantic interpretation of tool-generated data :**
    
    The AI reads the tool’s output incorrectly or misunderstands its meaning.
    
    → **Lack of due diligence in tool selection** : The AI chooses to use tools without checking if they are safe, trustworthy, or right for the task.
    
    → **Semantic or linguistic ambiguity in tool descriptions or naming :** The AI gets confused because the tool’s name or description is unclear or can be understood in different ways.
    
3. The operational efficacy of autonomous agents is predicated upon their robust utilization of tools for interfacing with external environments, executing computational tasks, and managing data flows.
    
4. The operational efficacy of autonomous agents is predicated upon their robust utilization of tools for interfacing with external environments, executing computational tasks, and managing data flows.
    
5. Consequently, inherent vulnerabilities within the agent's tool utilization paradigm present significant vectors for systemic compromise
    
6. Representing this risk we have a threat called **Tool Squatting** → a deceptive tactic used to exploit AI agents by tricking them into interacting with malicious tools or APIs